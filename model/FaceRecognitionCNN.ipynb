{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7df3786",
   "metadata": {},
   "source": [
    "# Face Recognition Using CNN Architecture in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c97ba69",
   "metadata": {},
   "source": [
    "## About the Dataset (Images)\n",
    "\n",
    "> The data contains cropped face images of 16 people divided into Training and testing. We will train the CNN model using the images in the Training folder and then test the model by using the unseen images from the testing folder, to check if the model is able to recognise the face number i.e label given to each face during training of the unseen images or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7ce5915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import tensorflow\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c614ab",
   "metadata": {},
   "source": [
    "# Image Agumentation with Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "127fef77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 244 images belonging to 16 classes.\n",
      "Found 244 images belonging to 16 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'face1': 0,\n",
       " 'face10': 1,\n",
       " 'face11': 2,\n",
       " 'face12': 3,\n",
       " 'face13': 4,\n",
       " 'face14': 5,\n",
       " 'face15': 6,\n",
       " 'face16': 7,\n",
       " 'face2': 8,\n",
       " 'face3': 9,\n",
       " 'face4': 10,\n",
       " 'face5': 11,\n",
       " 'face6': 12,\n",
       " 'face7': 13,\n",
       " 'face8': 14,\n",
       " 'face9': 15}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images = r\"Face Images/Face Images/Final Training Images\"\n",
    "\n",
    "## Image Preprocessing using keras\n",
    "\n",
    "\n",
    "# As we know deep-learning is hungry for data, the data we have is only limited. \n",
    "# so lets perform **Image Agumentation** to create different versions\n",
    "# of the original image, which leads to a better model, since it learns\n",
    "# on the good and bad mix of images.\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_gen = ImageDataGenerator(\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "# No transformations are made on the test data\n",
    "test_gen = ImageDataGenerator()\n",
    "\n",
    "# Generating training data\n",
    "training_data = train_gen.flow_from_directory(\n",
    "    train_images, \n",
    "    target_size = (100,100),\n",
    "    batch_size = 30,\n",
    "    class_mode = 'categorical'\n",
    ")\n",
    "\n",
    "# generating test data\n",
    "testing_data = test_gen.flow_from_directory(\n",
    "    train_images, \n",
    "    target_size = (100,100),\n",
    "    batch_size = 30,\n",
    "    class_mode = 'categorical'\n",
    ")\n",
    "\n",
    "# Printing class labels for each face\n",
    "testing_data.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264fe4e3",
   "metadata": {},
   "source": [
    "## Mapping of class_labels with numeric value for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bde407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data have numeric tag for each face\n",
    "Train_class = training_data.class_indices\n",
    "\n",
    "# lets store them in a dictionary with swap for future reference\n",
    "Result_class = {}\n",
    "for value_tag, face_tag in zip(Train_class.values(),Train_class.keys()):\n",
    "    Result_class[value_tag] = face_tag\n",
    "\n",
    "    \n",
    "# use pickle to save the mapping's\n",
    "import pickle\n",
    "with open(r'Face Images/Face Images/ResultMap.pkl','wb') as Final_mapping:\n",
    "    pickle.dump(Result_class,Final_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe1ce27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of Face and its numeric value {0: 'face1', 1: 'face10', 2: 'face11', 3: 'face12', 4: 'face13', 5: 'face14', 6: 'face15', 7: 'face16', 8: 'face2', 9: 'face3', 10: 'face4', 11: 'face5', 12: 'face6', 13: 'face7', 14: 'face8', 15: 'face9'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Mapping of Face and its numeric value\",Result_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dc02d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The Number of output neurons:  16\n"
     ]
    }
   ],
   "source": [
    "Output_Neurons=len(Result_class)\n",
    "print('\\n The Number of output neurons: ', Output_Neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5135e5f5",
   "metadata": {},
   "source": [
    "# Building the CNN Architecture, Model Compilation and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f06e9",
   "metadata": {},
   "source": [
    "**In the below code snippet, I have created a CNN model with**\n",
    "\n",
    "3. hidden layers of convolution\n",
    "3. hidden layers of max pooling\n",
    "1. layer of flattening\n",
    "2. Hidden ANN layer\n",
    "1. output layer with 16-neurons (one for each face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "151a98db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e8a8a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "9/9 [==============================] - 3s 245ms/step - loss: 238.5195 - accuracy: 0.0697 - val_loss: 48.7268 - val_accuracy: 0.1230\n",
      "Epoch 2/30\n",
      "9/9 [==============================] - 2s 204ms/step - loss: 18.2782 - accuracy: 0.1680 - val_loss: 3.3543 - val_accuracy: 0.3566\n",
      "Epoch 3/30\n",
      "9/9 [==============================] - 2s 207ms/step - loss: 2.5552 - accuracy: 0.3320 - val_loss: 2.1222 - val_accuracy: 0.3934\n",
      "Epoch 4/30\n",
      "9/9 [==============================] - 2s 207ms/step - loss: 2.0935 - accuracy: 0.3607 - val_loss: 1.8493 - val_accuracy: 0.4754\n",
      "Epoch 5/30\n",
      "9/9 [==============================] - 2s 263ms/step - loss: 1.9565 - accuracy: 0.4057 - val_loss: 1.5400 - val_accuracy: 0.5656\n",
      "Epoch 6/30\n",
      "9/9 [==============================] - 2s 249ms/step - loss: 1.5350 - accuracy: 0.5615 - val_loss: 1.1531 - val_accuracy: 0.6475\n",
      "Epoch 7/30\n",
      "9/9 [==============================] - 2s 244ms/step - loss: 1.1252 - accuracy: 0.6844 - val_loss: 0.8388 - val_accuracy: 0.7951\n",
      "Epoch 8/30\n",
      "9/9 [==============================] - 2s 242ms/step - loss: 0.7813 - accuracy: 0.8074 - val_loss: 0.5537 - val_accuracy: 0.8443\n",
      "Epoch 9/30\n",
      "9/9 [==============================] - 2s 214ms/step - loss: 0.5772 - accuracy: 0.8648 - val_loss: 0.3446 - val_accuracy: 0.9098\n",
      "Epoch 10/30\n",
      "9/9 [==============================] - 2s 221ms/step - loss: 0.4212 - accuracy: 0.8811 - val_loss: 0.2719 - val_accuracy: 0.9262\n",
      "Epoch 11/30\n",
      "9/9 [==============================] - 2s 214ms/step - loss: 0.3205 - accuracy: 0.8893 - val_loss: 0.1505 - val_accuracy: 0.9590\n",
      "Epoch 12/30\n",
      "9/9 [==============================] - 2s 237ms/step - loss: 0.1517 - accuracy: 0.9590 - val_loss: 0.1316 - val_accuracy: 0.9590\n",
      "Epoch 13/30\n",
      "9/9 [==============================] - ETA: 0s - loss: 0.1556 - accuracy: 0.9508\n",
      "Akurasi validasi mencapai 98.36%, menghentikan pelatihan!\n",
      "9/9 [==============================] - 2s 216ms/step - loss: 0.1556 - accuracy: 0.9508 - val_loss: 0.0534 - val_accuracy: 0.9836\n",
      "Total Training Time taken:  0 Minutes\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import Callback, EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense\n",
    "import time\n",
    "\n",
    "# Definisikan callback kustom untuk menghentikan pelatihan ketika akurasi > 96%\n",
    "class CustomEarlyStopping(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs.get('val_accuracy') >= 0.96:\n",
    "            print(f\"\\nAkurasi validasi mencapai {logs.get('val_accuracy') * 100:.2f}%, menghentikan pelatihan!\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "# Definisikan model\n",
    "Model = Sequential()\n",
    "Model.add(Conv2D(16, kernel_size=(5,5), strides=(1,1), input_shape=(100, 100, 3), activation='relu'))\n",
    "Model.add(MaxPool2D(pool_size=(2,2)))\n",
    "Model.add(Conv2D(32, kernel_size=(3,3), strides=(1,1), activation='relu'))\n",
    "Model.add(MaxPool2D(pool_size=(2,2)))\n",
    "Model.add(Flatten())\n",
    "Model.add(Dense(64, activation='relu'))\n",
    "Model.add(Dense(Output_Neurons, activation='softmax'))\n",
    "\n",
    "# Kompilasi model\n",
    "Model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Menggunakan Early stopping untuk mengurangi waktu pelatihan\n",
    "call = CustomEarlyStopping()\n",
    "\n",
    "# Mengukur waktu yang dibutuhkan oleh model untuk melatih\n",
    "StartTime = time.time()\n",
    "\n",
    "# Melatih model\n",
    "Model.fit(training_data,\n",
    "          epochs=30,\n",
    "          validation_data=testing_data,\n",
    "          callbacks=[call])\n",
    "\n",
    "Endtime = time.time()\n",
    "print('Total Training Time taken: ', round((Endtime - StartTime) / 60), 'Minutes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "282a2ef2-c70a-4660-ac3c-16e221d163f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "Model.save('face_recognition_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db111fa",
   "metadata": {},
   "source": [
    "## Using the Classifier to make predictions on unseen test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6412c5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce515a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "Prediction is:  face1\n"
     ]
    }
   ],
   "source": [
    "'''########### Making single predictions ###########'''\n",
    "\n",
    "ImagePath=r\"Face Images/Face Images/Final Training Images/face1/image_0001_Face_1.jpg\"\n",
    "test_image=image.load_img(ImagePath,target_size=(100, 100))\n",
    "test_image=image.img_to_array(test_image)\n",
    " \n",
    "test_image=np.expand_dims(test_image,axis=0)\n",
    " \n",
    "result=Model.predict(test_image,verbose=0)\n",
    "#print(training_set.class_indices)\n",
    " \n",
    "print('####'*10)\n",
    "print('Prediction is: ',Result_class[np.argmax(result)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9bd6d66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Face Images/Face Images/Final Testing Images\\\\face1\\\\1face1.jpg', 'Face Images/Face Images/Final Testing Images\\\\face1\\\\2face1.jpg', 'Face Images/Face Images/Final Testing Images\\\\face1\\\\3face1.jpg', 'Face Images/Face Images/Final Testing Images\\\\face1\\\\4face1.jpg', 'Face Images/Face Images/Final Testing Images\\\\face10\\\\1face10.jpg']\n",
      "**************************************************\n",
      "Prediction:  face1\n",
      "Prediction:  face1\n",
      "Prediction:  face13\n",
      "Prediction:  face4\n",
      "Prediction:  face10\n",
      "Prediction:  face10\n",
      "Prediction:  face10\n",
      "Prediction:  face10\n",
      "Prediction:  face11\n",
      "Prediction:  face11\n",
      "Prediction:  face11\n",
      "Prediction:  face11\n",
      "Prediction:  face12\n",
      "Prediction:  face12\n",
      "Prediction:  face12\n",
      "Prediction:  face12\n",
      "Prediction:  face4\n",
      "Prediction:  face13\n",
      "Prediction:  face13\n",
      "Prediction:  face16\n",
      "Prediction:  face14\n",
      "Prediction:  face14\n",
      "Prediction:  face14\n",
      "Prediction:  face14\n",
      "Prediction:  face15\n",
      "Prediction:  face15\n",
      "Prediction:  face15\n",
      "Prediction:  face15\n",
      "Prediction:  face16\n",
      "Prediction:  face16\n",
      "Prediction:  face16\n",
      "Prediction:  face16\n",
      "Prediction:  face2\n",
      "Prediction:  face3\n",
      "Prediction:  face2\n",
      "Prediction:  face2\n",
      "Prediction:  face3\n",
      "Prediction:  face3\n",
      "Prediction:  face3\n",
      "Prediction:  face3\n",
      "Prediction:  face4\n",
      "Prediction:  face4\n",
      "Prediction:  face4\n",
      "Prediction:  face4\n",
      "Prediction:  face5\n",
      "Prediction:  face5\n",
      "Prediction:  face5\n",
      "Prediction:  face5\n",
      "Prediction:  face6\n",
      "Prediction:  face6\n",
      "Prediction:  face6\n",
      "Prediction:  face6\n",
      "Prediction:  face7\n",
      "Prediction:  face7\n",
      "Prediction:  face7\n",
      "Prediction:  face7\n",
      "Prediction:  face8\n",
      "Prediction:  face8\n",
      "Prediction:  face4\n",
      "Prediction:  face8\n",
      "Prediction:  face9\n",
      "Prediction:  face9\n",
      "Prediction:  face9\n",
      "Prediction:  face9\n"
     ]
    }
   ],
   "source": [
    "'''############ Making multiple predictions ###########'''\n",
    "\n",
    "## Loading all the image paths from final testing folder for prediction\n",
    "main_ = r\"Face Images/Face Images/Final Testing Images\"\n",
    "img_paths = glob.glob(os.path.join(main_,'**','*.jpg'))\n",
    "\n",
    "print(img_paths[0:5]) # every image will be a PIL object\n",
    "print('*'*50)\n",
    "\n",
    "for path in img_paths:\n",
    "    test_image = image.load_img(path,target_size=(100,100))\n",
    "    test_image = image.img_to_array(test_image)\n",
    "    test_image = np.expand_dims(test_image,axis =0)\n",
    "    result = Model.predict(test_image,verbose=0)\n",
    "    print('Prediction: ',Result_class[np.argmax(result)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
